<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>FreeRTOS-2</title>
    <url>/2023/09/02/FreeRTOS-2/</url>
    <content><![CDATA[<h1 id="任务操作相关的api函数">任务操作相关的API函数</h1>
<table>
<thead>
<tr class="header">
<th>API函数</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>xTaskCreate()</td>
<td>动态方式创建任务</td>
</tr>
<tr class="even">
<td>xTaskCreateStatic()</td>
<td>静态方式创建任务</td>
</tr>
<tr class="odd">
<td>xTaskDelete()</td>
<td>删除任务</td>
</tr>
</tbody>
</table>
<p>由xTaskCreate()创建的任务，在创建成功后立马进入就绪态。被删除的任务将从所有的列表删除。动态创建方式是由FreeRTOS进行内存管理，静态则是由人工进行管理。静态创建比较麻烦，因此大多使用动态创建。</p>
<p>若使用静态创建，需要将宏<code>configSUPPORT_STATIC_ALLOCATION</code>置为1。同样若为动态创建，则需将宏<code>configSUPPORT_DYNAMIC_ALLOCATION</code>置为1。</p>
<h2 id="xtaskcreate函数">xTaskCreate()函数</h2>
<p>函数原型</p>
<div data-align="center">
<p><img src="/2023/09/02/FreeRTOS-2/FreeRTOS-2-1.png" height="360"></p>
</div>
<p>形参描述</p>
<table>
<colgroup>
<col style="width: 15%">
<col style="width: 84%">
</colgroup>
<thead>
<tr class="header">
<th>形参</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>pxTaskCode</td>
<td>指向任务函数的指针，即任务函数的名字</td>
</tr>
<tr class="even">
<td>pcName</td>
<td>任务函数的名字</td>
</tr>
<tr class="odd">
<td>usStackDepth</td>
<td>任务堆栈大小，单位：字</td>
</tr>
<tr class="even">
<td>pvParameters</td>
<td>传递给任务函数的参数</td>
</tr>
<tr class="odd">
<td>uxPriority</td>
<td>任务函数优先级</td>
</tr>
<tr class="even">
<td>pxCreatedTask</td>
<td>任务句柄，任务成功创建后，会返回任务句柄。任务句柄就是任务的任务控制块</td>
</tr>
</tbody>
</table>
<p>返回值</p>
<table>
<thead>
<tr class="header">
<th>返回值</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>pdPASS</td>
<td>任务创建成功</td>
</tr>
<tr class="even">
<td>errCOULD_NOT_ALLOCATE_REQUIRED_MEMORY</td>
<td>内存不足，任务创建失败</td>
</tr>
</tbody>
</table>
<h2 id="xtaskcreatestatic函数">xTaskCreateStatic()函数</h2>
<p>函数原型</p>
<div data-align="center">
<p><img src="/2023/09/02/FreeRTOS-2/FreeRTOS-2-2.png" height="360"></p>
</div>
<p>形参描述</p>
<table>
<colgroup>
<col style="width: 12%">
<col style="width: 87%">
</colgroup>
<thead>
<tr class="header">
<th>形参</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>pxTaskCode</td>
<td>指向任务函数的指针，即任务函数的名字</td>
</tr>
<tr class="even">
<td>pcName</td>
<td>任务函数的名字</td>
</tr>
<tr class="odd">
<td>ulStackDepth</td>
<td>任务堆栈大小，单位：字</td>
</tr>
<tr class="even">
<td>pvParameters</td>
<td>传递给任务函数的参数</td>
</tr>
<tr class="odd">
<td>uxPriority</td>
<td>任务函数优先级</td>
</tr>
<tr class="even">
<td>puxStackBuffer</td>
<td>任务栈指针，内存由用户分配提供,就是定义一个数组，数组的名字就是这个指针，数组的大小就是任务堆栈大小</td>
</tr>
<tr class="odd">
<td>pxTaskBuffer</td>
<td>任务控制块指针，内存由用户分配提供</td>
</tr>
<tr class="even">
<td>pxCreatedTask</td>
<td>任务句柄，任务成功创建后，会返回任务句柄。任务句柄就是任务的任务控制块</td>
</tr>
</tbody>
</table>
<p>返回值</p>
<table>
<thead>
<tr class="header">
<th>返回值</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>NULL</td>
<td>用户没有提供相应的内存，任务创建失败</td>
</tr>
<tr class="even">
<td>其他值</td>
<td>任务句柄，创建成功</td>
</tr>
</tbody>
</table>
<h2 id="vtaskdelete函数">vTaskDelete()函数</h2>
<table>
<thead>
<tr class="header">
<th>形参</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>vTaskToDelete</td>
<td>待删除的任务的句柄</td>
</tr>
</tbody>
</table>
<p>当传入的实参为<code>NULL</code>时，代表删除任务自身</p>
<p>该函数无返回值</p>
<h1 id="命名规范">命名规范</h1>
<ul>
<li>u: unsigned</li>
<li>s: short</li>
<li>l: long</li>
<li>c: char</li>
<li>x:
用户自定义的数据类型，如结构体，队列等。表示的类型为BaseType_t。如函数<code>xTaskCreate()</code>，其返回值就是BaseType_t类型</li>
<li>e: 枚举</li>
<li>p: 指针</li>
<li>prv: static函数</li>
<li>v: void函数，无返回值，如<code>vTaskDelete()</code></li>
</ul>
<p>函数名包含了该函数的返回值、函数所在的文件、函数功能。</p>
<p>如<code>xTaskCreateStatic()</code>，其返回值类型为BaseType_t，在task.c文件中，功能是静态创建任务。</p>
<p>参考：<code>https://blog.csdn.net/freestep96/article/details/126692753</code></p>
]]></content>
      <categories>
        <category>STM32</category>
      </categories>
      <tags>
        <tag>FreeRTOS</tag>
        <tag>STM32</tag>
        <tag>C语言</tag>
      </tags>
  </entry>
  <entry>
    <title>FreeRTOS-1</title>
    <url>/2023/08/27/FreeRTOS-1/</url>
    <content><![CDATA[<p>我是跟着正点原子的教程来学的，感觉讲的可以，因此很多知识点参考了正点原子</p>
<h1 id="为何使用freertos">为何使用FreeRTOS</h1>
<p>裸机在处理任务时，可以采用主函数while循环加中断处理函数的方法，但是这种方法实时性差，代码结构臃肿，并且当某个任务要执行delay()时，CPU只能干等，无法在这段时间去处理其他任务，不能充分利用CPU资源。</p>
<p>FreeRTOS为每个任务划分时间片，优先级。优先级相同的任务在执行时在划分的时间片内执行。在某个任务堵塞(delay)时，该任务进入堵塞状态，CPU可以转去执行其他的任务。</p>
<h1 id="freertos任务调度">FreeRTOS任务调度</h1>
<h2 id="任务调度器">任务调度器</h2>
<p>任务调度器就是通过某种调度算法决定要执行哪一个任务</p>
<h2 id="任务调度方式">任务调度方式</h2>
<ul>
<li>抢占式调度
<ul>
<li>高优先级抢占低优先级</li>
<li><strong>被抢占的任务会进入就绪态</strong></li>
<li>高优先级不结束，低优先级不能执行</li>
</ul></li>
<li>时间片调度
<ul>
<li><strong>优先级相同</strong>的任务会划分时间片，</li>
<li><strong>每个任务执行一个时间片</strong>，执行完后不断流转</li>
<li><strong>时间片的大小取决于系统滴答定时器</strong></li>
<li>倘若某个任务有一楼最好原因只执行0.2个时间片，那么在后面的运行中这个缺少的时间片并不会补回来，还是按照一个时间片执行</li>
</ul></li>
<li>协程式调度</li>
</ul>
<h2 id="任务状态">任务状态</h2>
<ul>
<li>运行态：某个任务正在被执行，那么该任务就处于运行态。在STM32中，只有一个任务处于运行态</li>
<li>就绪态：某个任务可以被执行，但是还未被执行，那么就处于就绪态</li>
<li>阻塞态：某任务在等待延时或者外部事件，则处于阻塞态</li>
<li>挂起态：类似于暂停，使用函数vTaskSuspend()挂起，使用vTaskResume()解挂，进入就绪态</li>
</ul>
<div data-align="center">
<p><img src="/2023/08/27/FreeRTOS-1/FreeRTOS-1-1.png" height="360"></p>
</div>
<p>由上图可知，<strong>仅有就绪态可以转变成运行态；任何任务想要转变为运行态，都要先转变成就绪态</strong></p>
<h2 id="任务列表">任务列表</h2>
<p>除运行态外，其他三种状态都有任务列表</p>
<ul>
<li>就绪列表</li>
</ul>
<table>
<thead>
<tr class="header">
<th>标志位</th>
<th>任务列表</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>bit31</td>
<td>pxReadyTasksLists[31]</td>
</tr>
<tr class="even">
<td>bit30</td>
<td>pxReadyTasksLists[30]</td>
</tr>
<tr class="odd">
<td>·····</td>
<td>·····················</td>
</tr>
<tr class="even">
<td>bit1</td>
<td>pxReadyTasksLists[1]</td>
</tr>
<tr class="odd">
<td>bit0</td>
<td>pxReadyTasksLists[0]</td>
</tr>
</tbody>
</table>
<p>标志位置一表示该就绪态优先级列表有任务，反之无任务。</p>
<p>数字越大，优先级等级越高。这一点与STM32中断优先级的高低不一样。</p>
<p>任务调度器就是在就绪列表中选择优先级最高的任务来执行，即进入运行态。</p>
<ul>
<li>阻塞列表：pxDelayedTaskList</li>
<li>挂起列表：xSuspendedTaskList</li>
</ul>
]]></content>
      <categories>
        <category>STM32</category>
      </categories>
      <tags>
        <tag>FreeRTOS</tag>
        <tag>STM32</tag>
        <tag>C语言</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习1</title>
    <url>/2023/08/12/MachineLearning-1/</url>
    <content><![CDATA[<h1 id="一些基本概念">一些基本概念</h1>
<h2 id="张量">张量</h2>
<p>pytorch中的基本数据结构，可以理解为多维数组。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.ones(<span class="number">3</span>)  <span class="comment"># 创建一个大小为3的一维张量，用1.0填充</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(a))</span><br><span class="line">a[<span class="number">2</span>] = <span class="number">55</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.tensor([[<span class="number">2</span>, <span class="number">3</span>], [<span class="number">3</span>, <span class="number">4</span>]])  <span class="comment"># 创建一个2维张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(a))</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">55</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a.shape)  <span class="comment">#查看每个维度上张量的大小</span></span><br></pre></td></tr></table></figure>
<p>可以通过从0开始的索引来访问张量中的每一个元素，也可以修改值。</p>
<p>切片的方法也适用于张量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(a[<span class="number">1</span>:])    <span class="comment">#第一行之后的所有行，所有列</span></span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">1</span>:,:])  <span class="comment">#第一行之后的所有行，所有列</span></span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">1</span>:,<span class="number">0</span>])  <span class="comment">#第一行之后的所有行，第一列</span></span><br><span class="line"><span class="built_in">print</span>(a[<span class="literal">None</span>])  <span class="comment">#增加大小为一的维度，类似于unsqueeze()方法</span></span><br></pre></td></tr></table></figure>
<p><strong>张量的大小、偏移量、步长</strong>。张量的大小是一个元组，表示张量在每个维度上有多少个元素；偏移量是指存储区中某个元素相对于张量中第一个元素的索引；步长是指存储区中为了获得下一个元素需要跳过的元素数量，它是一个元组，指示当索引在每个维度增加1时在储存区中要跳过的元素数量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.tensor(</span><br><span class="line">    [[[<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">6</span>, <span class="number">7</span>], [<span class="number">2</span>, <span class="number">6</span>, <span class="number">8</span>]], [[<span class="number">13</span>, <span class="number">32</span>, <span class="number">11</span>], [<span class="number">41</span>, <span class="number">46</span>, <span class="number">7</span>], [<span class="number">52</span>, <span class="number">65</span>, <span class="number">84</span>]]]</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(a.size())</span><br><span class="line">point = a[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line"><span class="built_in">print</span>(point)</span><br><span class="line"><span class="built_in">print</span>(point.storage_offset())  <span class="comment"># 该元素相对于第一个元素的偏移</span></span><br><span class="line"><span class="built_in">print</span>(a.stride())  <span class="comment"># 步长</span></span><br><span class="line"><span class="built_in">print</span>(a.dim())  <span class="comment">#查看张量的维度</span></span><br></pre></td></tr></table></figure>
<p>需要注意的是，通过索引的方式去更改子张量，会影响原始张量，这是因为子张量与原始张量索引了相同的存储区。可以使用clone()方法复制新的张量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.tensor(</span><br><span class="line">    [[[<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">6</span>, <span class="number">7</span>], [<span class="number">2</span>, <span class="number">6</span>, <span class="number">8</span>]], [[<span class="number">13</span>, <span class="number">32</span>, <span class="number">11</span>], [<span class="number">41</span>, <span class="number">46</span>, <span class="number">7</span>], [<span class="number">52</span>, <span class="number">65</span>, <span class="number">84</span>]]]</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(a.size())</span><br><span class="line">point = a[<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line"><span class="built_in">print</span>(point)</span><br><span class="line">point[<span class="number">1</span>] = <span class="number">111</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">point1 = point.clone()  <span class="comment"># 复制新的张量</span></span><br><span class="line"><span class="built_in">print</span>(point1)</span><br><span class="line">point1[<span class="number">1</span>] = <span class="number">222</span></span><br><span class="line"><span class="built_in">print</span>(point1)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></table></figure>
<p>此外，张量还有其他操作。如：转置t()方法、连续contiguous()方法。张量还具有设备属性，可以指定在CPU或者GPU上创建张量。可以将张量保存(save()方法)，也可以加载本地张量(load()方法)。可以通过<code>h5py</code>库，将张量转化成NumPy数组，这样可以适用于不同的库。</p>
<h1 id="线性模型">线性模型</h1>
<h2 id="机器学习machine-learning几个步骤">机器学习(machine
learning)几个步骤</h2>
<ol type="1">
<li><p><strong>准备数据集(dataset)</strong></p>
<p>将我们的数据转变成张量(tensor)，一般使用torch.utils.data包下的Dataset类中的API接口。还有加载数据集，一般是使用DataLoader类。有时还需要对数据集进行预处理，如下。</p>
<p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">form torchvision <span class="keyword">import</span> transforms</span><br><span class="line">preprocess = transforms.Compose([</span><br><span class="line">    transforms.Resize(<span class="number">256</span>),</span><br><span class="line">    transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(</span><br><span class="line">        mean=[<span class="number">0.485</span>,<span class="number">0.456</span>,<span class="number">0.406</span>],</span><br><span class="line">        std=[<span class="number">0.229</span>,<span class="number">0.224</span>,<span class="number">0.225</span>]</span><br><span class="line">    )</span><br><span class="line">]</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<p>这个预处理函数的意思是：将输入图像缩放到256<em>256，围绕中心将图像裁剪为224</em>224个像素，将其转换成张量，对其RGB分量进行归一化处理，使其具有定义的均值和标准差。</p></li>
<li><p><strong>选择模型(model)。例如pytorch提供的cv相关的模型</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">dir</span>(models))</span><br></pre></td></tr></table></figure>
<p>通过上述代码可以查看cv相关的模型。</p></li>
<li><p><strong>训练(training)</strong></p></li>
<li><p><strong>应用(inferring)</strong></p></li>
</ol>
<p>数据集分为两部分：训练集、测试集。</p>
<p>训练集又可以细分为训练集、开发集</p>
<p>损失函数是针对一个样本的,平均平方误差(Mean Square
Error,mse)是针对于整个训练集。</p>
<p>训练神经网络本质上就是使用几个或者一些参数将一个模型变换为更加复杂的模型</p>
<p>损失函数的选择很重要，因为它是一种对训练样本中要修正的错误进行优先处理的方法，可以强调或者忽略某些误差</p>
<p>优化器：torch.optiom中提供，用于更新</p>
<h2 id="过拟合">过拟合</h2>
<p>用训练集去训练模型，在尽可能的使损失最小后，将模型在测试集验证时发现，模型产生的损失比预期的要高得多，即过拟合</p>
<div data-align="center">
<p><img src="/2023/08/12/MachineLearning-1/MachineLearning-1-3.jpg" height="360"> 引自《Deep
Learning with PyTorch》</p>
</div>
<p>解决过拟合的方法 1.
在损失函数中添加惩罚项，以降低模型的成本，使其表现得更加平稳、变换更缓慢
2.
在输入样本中添加噪声，人为地在训练数据样本之间创建新的数据点，并使模型也拟合这些点
3. ···</p>
<p>那么现在可以将训练神经网络（选择合适参数）的过程分为两步：增大参数直到拟合；缩小参数以避免出现过拟合</p>
<h2 id="损失函数">损失函数</h2>
<h2 id="练习">练习</h2>
<h3 id="question">Question</h3>
<p>Suppose that students would get y points in final exam, if they spent
x hours in study</p>
<table>
<thead>
<tr class="header">
<th>x</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>2</td>
</tr>
<tr class="even">
<td>2</td>
<td>4</td>
</tr>
<tr class="odd">
<td>3</td>
<td>6</td>
</tr>
<tr class="even">
<td>4</td>
<td>?</td>
</tr>
</tbody>
</table>
<h3 id="answer">Answer</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">	</span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">x_data = &#123;<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>&#125;</span><br><span class="line">y_data = &#123;<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x, y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) * (y_pred - y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#权重及其对应损失值</span></span><br><span class="line">w_list = []</span><br><span class="line">mse_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> np.arange(<span class="number">0.0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;w=&#x27;</span>,w)</span><br><span class="line">    l_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        y_pred_val = forward(x_val)</span><br><span class="line">        loss_val = loss(x_val, y_val)</span><br><span class="line">        l_sum += loss_val</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\t&#x27;</span>, x_val, y_val, y_pred_val, loss_val)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;MSE=&#x27;</span>, l_sum/<span class="number">3</span>)</span><br><span class="line">    w_list.append(w)</span><br><span class="line">    mse_list.append(l_sum/<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(w_list, mse_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div data-align="center">
<p><img src="/2023/08/12/MachineLearning-1/MachineLearning-1-1.png" height="360" title="y=x*w" alt="y=x*w">
y=x*w</p>
</div>
<hr>
<h3 id="question-1">Question</h3>
<p>Suppose that students would get y points in final exam, if they spent
x hours in study.Try to use the model y=x*w+b, and draw the cost
graph.</p>
<table>
<thead>
<tr class="header">
<th>x</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>2</td>
</tr>
<tr class="even">
<td>2</td>
<td>4</td>
</tr>
<tr class="odd">
<td>3</td>
<td>6</td>
</tr>
<tr class="even">
<td>4</td>
<td>?</td>
</tr>
</tbody>
</table>
<h3 id="answer-1">Answer</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">x_data = &#123;<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>&#125;</span><br><span class="line">y_data = &#123;<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * w + b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x, y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) * (y_pred - y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 权重及其对应损失值</span></span><br><span class="line">w_list = []</span><br><span class="line">b_list = []</span><br><span class="line">mse_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> np.arange(<span class="number">0.0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>):</span><br><span class="line">    <span class="keyword">for</span> b <span class="keyword">in</span> np.arange(-<span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">0.1</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;w=&#x27;</span>,  w, <span class="string">&#x27;b=&#x27;</span>, b)</span><br><span class="line">        l_sum = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">            y_pred_val = forward(x_val)</span><br><span class="line">            loss_val = loss(x_val, y_val)</span><br><span class="line">            l_sum += loss_val</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;\t&#x27;</span>, x_val, y_val, y_pred_val, loss_val)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;MSE=&#x27;</span>, l_sum/<span class="number">3</span>)</span><br><span class="line">        w_list.append(w)</span><br><span class="line">        b_list.append(b)</span><br><span class="line">        mse_list.append(l_sum/<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mse_list)</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax3d = fig.add_subplot(projection=<span class="string">&#x27;3d&#x27;</span>)  <span class="comment"># 创建三维坐标系</span></span><br><span class="line">ax3d.plot_trisurf(w_list, b_list, mse_list)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div data-align="center">
<p><img src="/2023/08/12/MachineLearning-1/MachineLearning-1-2.png" height="360" title="y=x*w+b" alt="y=x*w+b">
y=x*w+b</p>
</div>
<hr>
<p>matplotlib中的函数还不怎么会用，后面抽个时间看一看</p>
<blockquote>
<p>https://blog.csdn.net/hustlei/article/details/122408179</p>
</blockquote>
<p>这个博客里面思维导图可以看一看捏</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习2</title>
    <url>/2023/08/13/MachineLearning-2/</url>
    <content><![CDATA[<h1 id="梯度下降算法">梯度下降算法</h1>
<hr>
<p>采用分治的思路，不断划分区块，进行搜索，但是会有可能陷入局部最优解</p>
<p>梯度：目标函数对权重求导。导数为负的方向就是最小指的方向</p>
<p>在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率
在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向</p>
<p>即：<code>ω = ω + α*θ</code>。θ为梯度，α为学习率或步长</p>
<p>类似于贪心算法，只看眼前最好的选择，不一定能得到最优结果，但能得到局部最优结果</p>
<p>鞍点：导数为零</p>
<hr>
<h2 id="随机梯度下降算法">随机梯度下降算法</h2>
<p>梯度下降衍生版本：随机梯度下降（stochastic gradient
descent）。随机选择单个样本的损失函数求导。可以避免陷入鞍点。SGD算法是从样本中随机抽出一组，训练后按梯度更新一次，然后再抽取一组，再更新一次，在样本量及其大的情况下，可能不用训练完所有的样本就可以获得一个损失值在可接受范围之内的模型了。</p>
<hr>
<p>针对于MachineLearning-1的问题，梯度下降算法代码如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">x_data = &#123;<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>&#125;</span><br><span class="line">y_data = &#123;<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始权重猜测</span></span><br><span class="line">w = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cost</span>(<span class="params">xs, ys</span>):</span><br><span class="line">    cost = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(xs, ys):</span><br><span class="line">        y_pred = forward(x)</span><br><span class="line">        cost += (y_pred - y) ** <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> cost / <span class="built_in">len</span>(xs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">xs, ys</span>):</span><br><span class="line">    grad = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(xs, ys):</span><br><span class="line">        grad += <span class="number">2</span> * x * (x * w - y)</span><br><span class="line">    <span class="keyword">return</span> grad / <span class="built_in">len</span>(xs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict (before training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    cost_val = cost(x_data, y_data)</span><br><span class="line">    grad_val = gradient(x_data, y_data)</span><br><span class="line">    w -= <span class="number">0.01</span> * grad_val</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch:&#x27;</span>, epoch, <span class="string">&#x27;w=&#x27;</span>, w, <span class="string">&#x27;loss=&#x27;</span>, cost_val)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict (after training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<p>随机梯度下降代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">x_data = &#123;<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>&#125;</span><br><span class="line">y_data = &#123;<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始权重猜测</span></span><br><span class="line">w = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x, y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">xs, ys</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * x * (x * w - y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict (before training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        grad = gradient(x_data, y_data)</span><br><span class="line">        w -= <span class="number">0.01</span> * grad</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\tgrad:&#x27;</span>, x, y, grad)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;progress:&#x27;</span>, epoch, <span class="string">&#x27;w=&#x27;</span>, w, <span class="string">&#x27;loss=&#x27;</span>, loss(x, y))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict (after training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<p>梯度下降算法可以并行化，随机梯度下降不行</p>
<p>mini-batch：小批量随机梯度下降。如果数据样本的数量很大，那么一轮的迭代会很耗时间，所以可以把这些数据样本划分为若干份，这些子集就叫做mini—batch</p>
<p>有关随机梯度下降文章 &gt;
https://blog.csdn.net/qq_58146842/article/details/121280968?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86&amp;utm_medium=distribute.pc_search_result.none-task-blog-2<sub>all</sub>sobaiduweb~default-1-121280968.nonecase&amp;spm=1018.2226.3001.4187</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习3</title>
    <url>/2023/08/17/MachineLearning-3/</url>
    <content><![CDATA[<h1 id="反向传播back-propagation">反向传播（Back Propagation）</h1>
<p>权重太多，求解析式十分复杂，因此在面对复杂网络时，尝试把网络看作是一个图，根据链式法则(chain
rule)，求其解析式，即反向传播算法。</p>
<p>Back Propagation链式求导过程 1. Create Computational Graph 2. Local
Gradient 3. Given gradient from successive node 4. Use chain rule to
compute the gradient(Backward)</p>
<p>Tensor
张量，pytorch里的重要组成，可以是标量，也可以是向量，矩阵。它包含数据（data），导数（grad），即权重与损失函数对权重的导数。</p>
<p>正向的目标是求出本次的损失，反向的目标则是求出导数。蓝色为正向，红色为反向</p>
<div data-align="center">
<p><img src="/2023/08/17/MachineLearning-3/MachineLearning-3-1.png" height="360"></p>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个Tensor变量，权重</span></span><br><span class="line">w = torch.Tensor([<span class="number">1.0</span>])</span><br><span class="line"><span class="comment"># 需要计算梯度</span></span><br><span class="line">w.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型，此时w是Tensor，*要计算Tensor与Tensor之间的乘法，所以有一个对x的自动类型强转</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数，构建计算图</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x, y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;predict (before training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br><span class="line"><span class="comment"># w.grad也是个梯度，w.grad.item才是标量</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        l = loss(x, y)  <span class="comment"># l是个张量，前馈过程，计算loss</span></span><br><span class="line">        l.backward()<span class="comment">#计算梯度</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\tgrad:&quot;</span>, x, y, w.grad.item())</span><br><span class="line">        w.data = w.data - <span class="number">0.01</span> * w.grad.data<span class="comment">#梯度用于更新权重</span></span><br><span class="line">        w.grad.data.zero_()  <span class="comment"># 梯度清零</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;process:&quot;</span>, epoch, l.item())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;predict(after training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br></pre></td></tr></table></figure>
<p>需要注意的是每一次通过.backward()计算的梯度会累积，因此在更新后，需要通过.grad.data.zero()将梯度置零</p>
<p>权重是指w，梯度是指loss对w的导数。要先通过前向过程求出loss，求出loss后通过反向传播求出loss对w的导数,再根据随机梯度下降算法或者其他算法对权重w进行更新。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习4</title>
    <url>/2023/08/20/MachineLearning-4/</url>
    <content><![CDATA[<h1 id="用pytorch实现线性回归linear-regression">用pytorch实现线性回归(linear
regression)</h1>
<p>使用梯度下降关键是求出损失函数对于权重的导数</p>
<h2 id="步骤">步骤</h2>
<ol type="1">
<li>准备数据集</li>
<li>设计模型</li>
<li>构建损失函数、优化器</li>
<li>训练周期</li>
</ol>
<h2 id="训练的步骤">训练的步骤</h2>
<ol type="1">
<li>求y^（预测结果）</li>
<li>求损失函数</li>
<li>backward</li>
<li>更新</li>
</ol>
<p>问题同前几篇blog</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">2.0</span>], [<span class="number">4.0</span>], [<span class="number">6.0</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># 构造对象，包含权重与偏置</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y_pred = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = LinearModel()<span class="comment">#可调用</span></span><br><span class="line"><span class="comment">#损失值应该是标量</span></span><br><span class="line">criterion = torch.nn.MSELoss(size_average=<span class="literal">False</span>)  <span class="comment"># false为不求平均值，即不除以N</span></span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)  <span class="comment"># lr,学习率</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    <span class="built_in">print</span>(epoch, loss.item())</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()  <span class="comment"># 进行更新</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;w = &quot;</span>, model.linear.weight.item())  <span class="comment"># weight是矩阵</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b = &quot;</span>, model.linear.bias.item())</span><br><span class="line"></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_pred = &quot;</span>, y_test.data)</span><br></pre></td></tr></table></figure>
<p>类名必须用大驼峰命名</p>
<p>要把模型定义成一个类，模型类都要继承于torch.nn.Model</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习5</title>
    <url>/2023/08/21/MachineLearning-5/</url>
    <content><![CDATA[<h1 id="逻辑斯蒂回归logistic-regression">逻辑斯蒂回归（logistic
regression)</h1>
<div data-align="center">
<p><img src="/2023/08/21/MachineLearning-5/MachineLearning-5-2.png" height="360"></p>
</div>
<p>名字虽然是回归，但实际上是分类</p>
<p>回归问题的预测结果范围是实数，而分类问题的预测结果为0~1，因此需要映射</p>
<p>可以使用Sigmoid函数(s型函数)，将线性模型的结果压缩，比如：</p>
<p><span class="math display">\[
f(x)=\frac{1}{1+{\rm e}^{-x}}
\]</span></p>
<p>那么预测结果(0~1)就为</p>
<p><span class="math display">\[
\hat{y} = f(w·x+b)=\frac{1}{1+{\rm e}^{-(w·x+b)}}
\]</span></p>
<div data-align="center">
<p><img src="/2023/08/21/MachineLearning-5/MachineLearning-5-1.png" height="360"></p>
</div>
<!--
## 逻辑斯蒂回归模型

<div align=center>
<img src="MachineLearning-5-3.png" height = '360'>
</div>

最后的判别结果是通过比较P(Y=1|x)和P(Y=0|x)的大小来确定的，若Y=1大，那么Y=1，反之Y=0

<p /hidden>
## 损失函数
<div align=center>
<img src="MachineLearning-5-5.png" height = '360'>
</div>

<div align=center>
<img src="MachineLearning-5-4.png" height = '360'>
</div>

### 对数似然函数

似然函数可以理解成通过已知的结果去倒推出最大概率得到该结果的参数，即“模型已定，参数未知”。就比如已知一些数据，我们对这个模型使用正态分布，那么我们就可以求出与已知的数据对应最好的那条拟合曲线，求出均值μ，标准差σ。

对于逻辑斯蒂回归,近似看成二项分布

$$
p(Y=Y_i|x_i,w) = 
\begin{cases}
[π(x_i)]^{Y_i}      & Y_i=1 \\
[1-π(x_i)]^{1-Y_i}] & Y_i=0 \\
\end{cases}
$$

则有
$$
p(Y=Y_i|x_i,w) = π(x_i)^{Y_i}·[1-π(x_i)]^{1-Y_i}]
$$

当有m个样本时
$$
P = \prod_{i=1}^m [π(x_i)^{Y_i}·[1-π(x_i)]^{1-Y_i}]
$$

对其取对数

$$
\begin{aligned}
L(w) &= \log_{e}{\prod_{i=1}^m [π(x_i)^{Y_i}·[1-π(x_i)]^{1-Y_i}]} \\
&= \sum_{i=1}^m [ {Y_i} log_{e}π(x_i) + (1-Y_i)log_{e}[1-π(x_i)] ] 
\end{aligned}
$$
-->
<h2 id="损失函数">损失函数</h2>
<div data-align="center">
<p><img src="/2023/08/21/MachineLearning-5/MachineLearning-5-6.png" height="360"></p>
</div>
<p>损失函数不再使用MSE，采用下面的式子</p>
<ul>
<li>当预测结果 <span class="math inline">\(\hat{y}=1\)</span>，实际结果为<span class="math inline">\(1\)</span>时，<span class="math inline">\(loss=0\)</span></li>
<li>当预测结果 <span class="math inline">\(\hat{y}=0\)</span>，实际结果为<span class="math inline">\(1\)</span>时，<span class="math inline">\(loss=+\infty\)</span></li>
<li>当预测结果 <span class="math inline">\(\hat{y}=1\)</span>，实际结果为<span class="math inline">\(0\)</span>时，<span class="math inline">\(loss=-\infty\)</span></li>
<li>当预测结果 <span class="math inline">\(\hat{y}=0\)</span>，实际结果为<span class="math inline">\(0\)</span>时，<span class="math inline">\(loss=0\)</span></li>
</ul>
<h2 id="代码">代码</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">0</span>], [<span class="number">0</span>], [<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LogisticRegreesionModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LogisticRegreesionModel, self).__init__()</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># 构造对象，包含权重与偏置</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y_pred = F.sigmoid(self.linear(x))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = LogisticRegreesionModel()</span><br><span class="line"></span><br><span class="line">criterion = torch.nn.BCELoss(size_average=<span class="literal">False</span>)  <span class="comment"># false为不求平均值，即不除以N</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)  <span class="comment"># 优化器 lr,学习率</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    <span class="built_in">print</span>(epoch, loss.item())</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()  <span class="comment"># 进行更新</span></span><br><span class="line"></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_pred = &quot;</span>, y_test.data)</span><br></pre></td></tr></table></figure>
<p>如果用matplotlib绘图，也可以看见输出结果<span class="math inline">\(\hat{y}\)</span>为S型曲线</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习6</title>
    <url>/2023/08/29/MachineLearning-6/</url>
    <content><![CDATA[<h1 id="多维输入">多维输入</h1>
<p>对于标量或者一维的回归任务，其回归模型可以表示为<span class="math inline">\(\hat{y}=\sigma(x^{(i)}·w+b)\)</span>。其中<span class="math inline">\(\sigma\)</span>是激活函数，类似于sigmoid函数，i为第i个样本</p>
<p>但对于多维的数据，例如有这么一个样本</p>
<table>
<thead>
<tr class="header">
<th>x1</th>
<th>x2</th>
<th>x3</th>
<th>x4</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>10</td>
<td>11</td>
<td>21</td>
<td>31</td>
<td>234</td>
</tr>
<tr class="even">
<td>10</td>
<td>11</td>
<td>21</td>
<td>31</td>
<td>234</td>
</tr>
<tr class="odd">
<td>10</td>
<td>11</td>
<td>21</td>
<td>31</td>
<td>234</td>
</tr>
</tbody>
</table>
<p>对于这么一组数据来说，x1、x2、x3、x4被称为<strong>特征(feature)</strong>。那么回归模型的输入就是一组向量:</p>
<p><span class="math display">\[
\left[
\begin{matrix}
    x_1 &amp; x_2 &amp;x_3 &amp; x_4
\end{matrix}
\right]
\]</span></p>
<p>回归模型变为<span class="math inline">\(\hat{y}=\sigma(\sum_{n=1}^{4}
x{_n^{(i)}}·w_n+b)=\sigma(z^{(i)})\)</span>，<span class="math inline">\(x{_n^{(i)}}\)</span>表示第i个样本的<span class="math inline">\(x_n\)</span>。其中的w·x虽然表示的是标量相乘，但是实际的含义是矩阵相乘，即</p>
<p><span class="math display">\[
\left[
\begin{matrix}
    x_1 &amp; x_2 &amp; x_3 &amp; x_4
\end{matrix}
\right]
\left[
\begin{matrix}
    w_1\\
    w_2\\
    w_3\\
    w_4\\
\end{matrix}
\right]
\]</span></p>
<p>计算结果依旧是标量</p>
<h1 id="对于mini-batchn-samples">对于mini-batch(N samples)</h1>
<p>回归模型依旧是<span class="math inline">\(\hat{y}=\sigma(\sum_{n=1}^{N}
x{_n^{(i)}}·w_n+b)=\sigma(z^{(i)})\)</span></p>
<p>现在有N个样本</p>
<p><span class="math display">\[
\left[
\begin{matrix}
    {\hat{y}}^{(1)} \\
    \vdots \\
    {\hat{y}}^{(N)} \\
\end{matrix}
\right] =
\left[
\begin{matrix}
    \sigma(z^{(1)}) \\
    \vdots \\
    \sigma(z^{(N)}) \\
\end{matrix}
\right] =
\sigma
\left[
\begin{matrix}
    z^{(1)} \\
    \vdots \\
    z^{(N)} \\
\end{matrix}
\right]
\]</span></p>
<p><span class="math display">\[
z^{(N)}=\left[
\begin{matrix}
    x^{(N)}_1 &amp; \cdots &amp; \cdots &amp; x^{(N)}_4
\end{matrix}
\right]
\left[
\begin{matrix}
    w_1 \\
    \vdots \\
    \vdots \\
    w_4
\end{matrix}
\right]+b
\]</span></p>
<p>计算结果为标量，转置不影响,其中4是feature数量</p>
<p>那么我们就可以得到</p>
<p><span class="math display">\[
\left[
\begin{matrix}
    z^{(1)} \\
    \vdots \\
    z^{(N)} \\
\end{matrix}
\right]=
\left[
\begin{matrix}
    x^{(1)}_1 &amp; \cdots &amp; x^{(1)}_4\\
    \vdots    &amp; \ddots &amp; \vdots   \\
    x^{(N)}_1 &amp; \cdots &amp; x^{(N)}_4
\end{matrix}
\right]
\left[
\begin{matrix}
    w_1 \\
    \vdots \\
    \vdots \\
    w_4
\end{matrix}
\right]+
\left[
\begin{matrix}
    b \\
    \vdots \\
    b\\
\end{matrix}
\right]
\]</span></p>
<p>设样本数量为N，feature的数量为n，则</p>
<table>
<thead>
<tr class="header">
<th>x</th>
<th>w</th>
<th>b</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>N×n</td>
<td>n×1</td>
<td>N×1</td>
</tr>
</tbody>
</table>
<p>转成向量化的计算后，可利用并行化计算，提高计算速率。</p>
<p>在改代码时，只需将<code>self.linear = torch.nn.Linear(1, 1)</code>改成<code>self.linear = torch.nn.Linear(n, 1)</code></p>
<p>如果是多层网络，那么只需将上一层的输出，作为下一层的输入。</p>
<div data-align="center">
<p><img src="/2023/08/29/MachineLearning-6/MachineLearning-6-1.png" height="360"></p>
</div>
<h1 id="实际问题">实际问题</h1>
<div data-align="center">
<p><img src="/2023/08/29/MachineLearning-6/MachineLearning-6-2.png" height="360"></p>
</div>
<p>X1~X8为糖尿病病人的一系列指标，Y表示一年后病情是否加重。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">xy = np.loadtxt(</span><br><span class="line">    <span class="string">&quot;D:\BaiduNetdiskDownload\PyTorch深度学习实践\diabetes.csv.gz&quot;</span>,</span><br><span class="line">    delimiter=<span class="string">&quot;,&quot;</span>,</span><br><span class="line">    dtype=np.float32,</span><br><span class="line">)</span><br><span class="line">x_data = torch.from_numpy(xy[:, :-<span class="number">1</span>])</span><br><span class="line">y_data = torch.from_numpy(xy[:, [-<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(<span class="number">8</span>, <span class="number">6</span>)  <span class="comment"># 8维到6维</span></span><br><span class="line">        self.linear2 = torch.nn.Linear(<span class="number">6</span>, <span class="number">4</span>)  <span class="comment"># 6维到4维</span></span><br><span class="line">        self.linear3 = torch.nn.Linear(<span class="number">4</span>, <span class="number">1</span>)  <span class="comment"># 4维到1维</span></span><br><span class="line">        self.activate = torch.nn.Sigmoid()</span><br><span class="line">        <span class="comment"># self.activate=torch.nn.ReLU()#采用不同的激活函数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.activate(self.linear1(x))</span><br><span class="line">        x = self.activate(self.linear2(x))</span><br><span class="line">        x = self.activate(self.linear3(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Model()</span><br><span class="line"></span><br><span class="line">criterion = torch.nn.BCELoss(size_average=<span class="literal">True</span>)  <span class="comment"># false为不求平均值，即不除以N</span></span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)  <span class="comment"># lr,学习率</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># 前馈</span></span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    <span class="built_in">print</span>(epoch, loss.item())</span><br><span class="line">    <span class="comment"># 反馈</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 更新</span></span><br><span class="line">    optimizer.step()  <span class="comment"># 进行更新</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x_test = torch.Tensor([[-<span class="number">0.29</span>, <span class="number">0.49</span>, <span class="number">0.18</span>, -<span class="number">0.29</span>, <span class="number">0.00</span>, <span class="number">0.00</span>, -<span class="number">0.53</span>, -<span class="number">0.03</span>]])</span><br><span class="line"></span><br><span class="line">y_test = model(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_pred = &quot;</span>, y_test.data)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>指针</title>
    <url>/2024/10/13/%E6%8C%87%E9%92%88/</url>
    <content><![CDATA[<h1 id="数组">数组</h1>
<h2 id="数组名">数组名</h2>
<p>源码 <figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> a[]=&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">6</span>&#125;;</span><br><span class="line">    <span class="type">int</span> *p1=&amp;a[<span class="number">0</span>];</span><br><span class="line">    <span class="type">int</span> *p2=&amp;a;</span><br><span class="line">    <span class="type">int</span> *p3=&amp;a[<span class="number">1</span>];</span><br><span class="line">    <span class="type">int</span> *p4=a;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;p1=&amp;a[0],%p\n&quot;</span>,p1);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;p2=&amp;a,%p\n&quot;</span>,p2);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;p3=&amp;a[1],%p\n&quot;</span>,p3);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;p4=a,%p\n&quot;</span>,p4);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> 输出 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">p1=&amp;a[0],0x7fffd4c3fb90</span><br><span class="line">p2=&amp;a,0x7fffd4c3fb90</span><br><span class="line">p3=&amp;a[1],0x7fffd4c3fb94</span><br><span class="line">p4=a,0x7fffd4c3fb90</span><br></pre></td></tr></table></figure> 但是在编译时会出现警告：
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">new.c: In function ‘main’:</span><br><span class="line">new.c:15:13: warning: initialization of ‘int *’ from incompatible pointer type ‘int (*)[5]’ [-Wincompatible-pointer-types]</span><br><span class="line">   15 |     int *p2=&amp;a;</span><br><span class="line">      |             ^</span><br></pre></td></tr></table></figure>
错误是类型不匹配。查了一下，是因为a是一个指针，类型是<code>int *</code>，而<code>&amp;a</code>是一个数组的地址，类型是<code>int (*)[5]</code>，不能直接赋值给<code>int *</code>指针，即使两者的内容是一样的。</p>
<p>由上可见：<code>&amp;a[0]</code>，<code>a</code>都是指向数组首地址的指针，<code>&amp;a</code>也是指向数组首地址的指针，但是类型是<code>int (*)[5]</code>；<code>a[1]</code>与*(a+1)是一样的。</p>
<h2 id="指针数组">指针数组</h2>
<p>源码 <figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">char</span> *a[]=&#123;<span class="string">&quot;one&quot;</span>,<span class="string">&quot;two&quot;</span>,<span class="string">&quot;three&quot;</span>&#125;;</span><br><span class="line">    <span class="type">char</span> *p1=a;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> 编译后会出现警告： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">new2.c:6:14: warning: initialization of ‘char *’ from incompatible pointer type ‘char **’ [-Wincompatible-pointer-types]</span><br><span class="line">    6 |     char *p1=a;</span><br><span class="line">      |              ^</span><br></pre></td></tr></table></figure>
错误·是类型不匹配。原因是<code>a</code>是一个指针，指向一个指针数组，这个指针数组里的内容也是指针，所以<code>a</code>是一个指向指针的指针，即二级指针，所以类型是<code>char **</code>，不能直接赋值给<code>char *</code>指针p1，将<code>char *p1=a</code>改为<code>char **p1=a</code>即可。</p>
<p>打印这个指针数组指向的字符串 <figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>  </span><br><span class="line">&#123;  </span><br><span class="line">    <span class="type">char</span> *a[]=&#123;<span class="string">&quot;one&quot;</span>,<span class="string">&quot;two&quot;</span>,<span class="string">&quot;three&quot;</span>&#125;;  </span><br><span class="line">    <span class="type">char</span> **p1=a;  </span><br><span class="line">    <span class="type">int</span> i;  </span><br><span class="line">    <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;<span class="number">3</span>;i++)  </span><br><span class="line">    &#123;  </span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%s\n&quot;</span>,a[i]);  </span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%s\n&quot;</span>,*p1[i]);  </span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%s\n&quot;</span>,*(p1+i));</span><br><span class="line">    &#125;  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure></p>
<p>编译后出现警告： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">new2.c:11:18: warning: format ‘%s’ expects argument of type ‘char *’, but argument 2 has type ‘int’ [-Wformat=]</span><br><span class="line">   11 |         printf(&quot;%s\n&quot;,*p1[i]);</span><br><span class="line">      |                 ~^    ~~~~~~~~</span><br><span class="line">      |                  |    |</span><br><span class="line">      |                  |    int</span><br><span class="line">      |                  char *</span><br><span class="line">      |                 %d</span><br></pre></td></tr></table></figure>
<code>*p1[i]</code>相当于是<code>*(*(p1+i))</code>，<code>(p1+0)</code>是一个指向字符串首地址的指针，那么<code>*(p1+0)</code>里的内容已经是字符串的首地址了，再解引用那么内容就是字符串里的首个字符，所以<code>*p1[i]</code>的内容就是字符串里的首个字符。</p>
<p>要打印字符串，用的是<code>%s</code>,后面的参数应该是这个字符串的首地址，所以出现警告。</p>
<p>可以改下程序验证一下：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>  </span><br><span class="line">&#123;  </span><br><span class="line">    <span class="type">char</span> *a[]=&#123;<span class="string">&quot;one&quot;</span>,<span class="string">&quot;two&quot;</span>,<span class="string">&quot;three&quot;</span>&#125;;  </span><br><span class="line">    <span class="type">char</span> **p1=a;  </span><br><span class="line">    <span class="type">int</span> i;  </span><br><span class="line">    <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;<span class="number">3</span>;i++)  </span><br><span class="line">    &#123;  </span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;a[i] %s\n&quot;</span>,a[i]);  </span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;*p1[i] %c\n&quot;</span>,*p1[i]);  </span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;*(p1+i) %s\n&quot;</span>,*(p1+i));</span><br><span class="line">    &#125;  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>
<p>那么打印的应该是这三个字符串的首个字符<code>o</code>，<code>t</code>，<code>t</code>。</p>
<p>输出 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">a[i] one</span><br><span class="line">*p1[i] o</span><br><span class="line">*(p1+i) one</span><br><span class="line">a[i] two</span><br><span class="line">*p1[i] t</span><br><span class="line">*(p1+i) two</span><br><span class="line">a[i] three</span><br><span class="line">*p1[i] t</span><br><span class="line">*(p1+i) three</span><br></pre></td></tr></table></figure> 确实如此。</p>
<h1 id="指针的大小">指针的大小</h1>
<p>指针存放的内容是地址，不同的系统的地址范围不同，与位数有关。32位系统下，指针大小为4字节，即32位，64位系统下，指针大小为8字节。</p>
]]></content>
      <categories>
        <category>C语言</category>
      </categories>
      <tags>
        <tag>C语言</tag>
      </tags>
  </entry>
  <entry>
    <title>OV2640</title>
    <url>/2023/10/08/OV2640/</url>
    <content><![CDATA[<h1 id="dcmi">DCMI</h1>
<ul>
<li><p>VSYNC:帧信号，摄像头输出，用于标记一帧数据的开始与结束</p></li>
<li><p>HREF:行中断信号，摄像头输出，用于标记一行数据的开始与结束</p></li>
<li><p>D0-D7(D0-D9):数据线。</p></li>
</ul>
<p>dcmi我是直接用cube配置的，比较简单。</p>
<p>用到的一些hal库dcmi函数</p>
<ul>
<li>__HAL_DCMI_ENABLE_IT(&amp;hdcmi,
DCMI_IT_FRAME);//使能dcmi帧中断</li>
<li>HAL_DCMI_Start_DMA(&amp;hdcmi, DCMI_MODE_SNAPSHOT,
(uint32_t)JpegBuffer, pictureBufferLength);
//以dma方式启动dcmi，我看dcmi源文件，似乎也提供了这一种启动方式，因此在配置dcmi的时候一定要配置dma</li>
<li>HAL_DCMI_Suspend(hdcmi); // 挂起 DCMI</li>
<li>HAL_DCMI_Stop(hdcmi); // 停止dcmi,DMA传输</li>
</ul>
<h1 id="sccbserial-camera-control-bus协议">SCCB(Serial Camera Control
Bus)协议</h1>
<p>SCCB与I2C协议类似，由两根线组成：SCL(SIO_C)(时钟线)、SDA(SIO_D)(数据线)。值得注意的是，在SCCB的手册(我看的是《OmniVision
Technologies Seril Camera Control Bus(SCCB)
Specification》)上，有说SCCB是有三根线的，另外一根SCCB_E<del>应该是作为片选信号</del>，用来表明信号的开始传输与结束传输，这种方式支持一个主机与多个从机连接。如果只用两根线的话，那就只能支持一主机一从机。</p>
<p>相同点：</p>
<ul>
<li>起始信号、停止信号</li>
<li>信号有效性(SDA只能在SCL为低电平时变化)</li>
</ul>
<p>不同点：</p>
<ul>
<li>在第九个周期，I2C的SDA内容为应答信号；SCCB的SDA在写周期为Don't-Care位，在读周期为NA位</li>
<li>SCCB传输单位：相</li>
</ul>
<p>在SCCB中，数据传输的最基本单位叫做<code>相(phase)</code>。一个相包含9个位，前八个位是八位连续数据，第九个位是<code>Don't Care</code>或者<code>NA</code>，这个到底是哪一个取决于是读还是写。</p>
<p>phase-1 ID
Address是指从机的地址，bit7-bit1是从机的地址，bit0是读写位(0-write
1-read)。Sub-address 是内存地址。</p>
<div data-align="center">
<p><img src="/2023/10/08/OV2640/OV2640-1.png" height="240"></p>
</div>
<p>写操作：三相写传输周期</p>
<ul>
<li>设备写通信地址，内存地址，所写数据</li>
</ul>
<div data-align="center">
<p><img src="/2023/10/08/OV2640/OV2640-2.png" height="240"></p>
</div>
<p>读操作</p>
<ul>
<li>两相写传输周期:设备写通信地址，内存地址</li>
</ul>
<div data-align="center">
<p><img src="/2023/10/08/OV2640/OV2640-3.png" height="120"></p>
</div>
<ul>
<li>两相读传输周期：设备读通信地址，所读数据</li>
</ul>
<div data-align="center">
<p><img src="/2023/10/08/OV2640/OV2640-4.png" height="120"></p>
</div>
<p>需要注意的是SCCB不支持连续读写，只能写一个数据读一个数据。</p>
<h1 id="需要注意的几点">需要注意的几点</h1>
<ol type="1">
<li><p>在通过SCCB往OV2640写数据时，尤其要注意<strong>pdown这个引脚要设置为低电平，即退出低功耗模式。</strong></p></li>
<li><p><strong>SDA引脚在写数据的时候应该设置为开漏模式</strong>，这一点在数据手册上有说明。如果是推挽，我测试的是不成功的。接受信息的时候，理当设置为输入模式，但是我设置为开漏输出，通过read_pin()函数(即读取GPIO口高低电平的函数)也能读取数据。但是有些博客说如果设置GPIO口为输出模式，虽然<strong>也可以读取到高低电平，但只适用于推挽模式，对开漏模式并不适用</strong>。至于为什么我设置为开漏模式也能读取到高低电平，原因还需要查一查。</p></li>
<li><p>如果想要看一看自己写的SCCB协议是否正确，可以通过读OV2640的产品ID寄存器和生产商ID寄存器，生产商ID可以在datasheet上面查到，产品ID查不到，我读出来的是0x2642，与正点原子读出来的是一样的。</p></li>
<li><p>OV2640的地址在手册上面写的是0x60或0x61，但实际用这两个地址来写读数据是错误的，应该用0x30。这一点是看正点原子的程序才知道的，原因等我问问技术客服。问了客服，应该是出厂版本问题。</p></li>
<li><p>SCCB是和I2C兼容的，可以用cube配置I2C实现SCCB协议。可以参考<code>https://gitee.com/studentwei/stm32h750vbt-cubmx</code>。强烈推荐，代码十分简洁。</p></li>
<li><p>这个点与OV2640无关，是有关于使用cube配置STM32H7的一个问题。H7的最高主频为480M，但是我一开始配置时，发现不管怎么配时钟树，虽然时钟树能显示480M，但是实际我用定时器输出PWM的频率只有400M。<strong>要将<code>RCC</code>下的<code>Product rivision</code>设置为<code>rev.V</code>才行，默认是rev.Y。务必注意！！！</strong></p></li>
</ol>
<div data-align="center">
<p><img src="/2023/10/08/OV2640/OV2640-5.png" height="180"></p>
</div>
<ol start="7" type="1">
<li>在拿到0V2640后一定要看一看模块上是否已经有了给XCLK提供时钟信号的晶振，这个信号时不可或缺的，如果没有，可以尝试用单片机发PWM当做时钟信号(这就是上面我说PWM频率为400M的原因)，XCLK信号频率一般为24M，应该可以为其他频率，具体要参看手册。这个方法我没有成功，但大概率是模块本身的问题，如果没有晶振可以试一下。</li>
</ol>
]]></content>
      <categories>
        <category>STM32</category>
      </categories>
      <tags>
        <tag>STM32</tag>
      </tags>
  </entry>
  <entry>
    <title>C语言里的一些小知识(随手记)</title>
    <url>/2024/10/15/C%E8%AF%AD%E8%A8%80%E9%87%8C%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E7%9F%A5%E8%AF%86-%E9%9A%8F%E6%89%8B%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="宏相关">宏相关</h1>
<h2 id="include与define"><code>#include</code>与<code>#define</code></h2>
<p>宏其实就是粘贴代码。宏定义的好处是可以减少代码重复，提高代码的可读性和可维护性。</p>
<p>比如：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> NAMES(X) X(TOM) X(JERRY) X(MIKE)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> PRINT(X) puts(#X<span class="string">&quot; hello!&quot;</span>);</span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    NAMES(PRINT);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的代码定义了两个宏：<code>NAMES</code> 和
<code>PRINT</code>。<code>NAMES</code> 接受一个参数
<code>X</code>，然后将
<code>X(TOM)</code>、<code>X(JERRY)</code>、<code>X(MIKE)</code>
展开为三个 <code>printf</code> 语句。<code>PRINT</code> 接受一个参数
<code>X</code>，然后将 <code>#X</code> 展开为字符串
<code>"X = %d\n"</code>，并用 <code>X</code> 作为参数。</p>
<p>除此之外，<code>#include</code>是用来包含头文件的宏，<code>#include &lt;stdio.h&gt;</code>的作用就是把头文件<code>stdio.h</code>里的内容全部复制粘贴到这个源文件中。</p>
<p>宏定义的展开是在预编译的阶段进行的，可以通过<code>gcc -E file.c</code>来查看预编译后的代码。上面代码预编译后的部分结果：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">extern</span> <span class="type">char</span> *<span class="title function_">ctermid</span> <span class="params">(<span class="type">char</span> *__s)</span> __<span class="title function_">attribute__</span> <span class="params">((__nothrow__ , __leaf__))</span></span><br><span class="line">  __<span class="title function_">attribute__</span> <span class="params">((__access__ (__write_only__, <span class="number">1</span>)))</span>;</span><br><span class="line"># <span class="number">867</span> <span class="string">&quot;/usr/include/stdio.h&quot;</span> <span class="number">3</span> <span class="number">4</span></span><br><span class="line"><span class="keyword">extern</span> <span class="type">void</span> <span class="title function_">flockfile</span> <span class="params">(FILE *__stream)</span> __<span class="title function_">attribute__</span> <span class="params">((__nothrow__ , __leaf__))</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">extern</span> <span class="type">int</span> <span class="title function_">ftrylockfile</span> <span class="params">(FILE *__stream)</span> __<span class="title function_">attribute__</span> <span class="params">((__nothrow__ , __leaf__))</span> ;</span><br><span class="line"></span><br><span class="line"><span class="keyword">extern</span> <span class="type">void</span> <span class="title function_">funlockfile</span> <span class="params">(FILE *__stream)</span> __<span class="title function_">attribute__</span> <span class="params">((__nothrow__ , __leaf__))</span>;</span><br><span class="line"># <span class="number">885</span> <span class="string">&quot;/usr/include/stdio.h&quot;</span> <span class="number">3</span> <span class="number">4</span></span><br><span class="line"><span class="keyword">extern</span> <span class="type">int</span> __uflow (FILE *);</span><br><span class="line"><span class="keyword">extern</span> <span class="type">int</span> __overflow (FILE *, <span class="type">int</span>);</span><br><span class="line"># <span class="number">902</span> <span class="string">&quot;/usr/include/stdio.h&quot;</span> <span class="number">3</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line"># <span class="number">10</span> <span class="string">&quot;new6.c&quot;</span> <span class="number">2</span></span><br><span class="line"></span><br><span class="line"># <span class="number">14</span> <span class="string">&quot;new6.c&quot;</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">puts</span>(<span class="string">&quot;TOM&quot;</span><span class="string">&quot; hello!&quot;</span>); <span class="built_in">puts</span>(<span class="string">&quot;JERRY&quot;</span><span class="string">&quot; hello!&quot;</span>); <span class="built_in">puts</span>(<span class="string">&quot;MIKE&quot;</span><span class="string">&quot; hello!&quot;</span>);;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>前面那一部分是<code>stdio.h</code>中的内容，在<code>main</code>主函数中可以看到，相关的宏都已经被展开了。</p>
<h2 id="include-header.h和include-header.h"><code>include &lt;header.h&gt;</code>和<code>#include "header.h"</code></h2>
<p><code>#include &lt;header.h&gt;</code>是系统头文件，位于系统目录下，一般是由操作系统提供的。</p>
<p><code>#include "header.h"</code>是用户头文件，位于当前目录下，一般是由程序员自己编写的。</p>
<p>可以通过<code>gcc --verbose src.c</code>来查看头文件搜索路径。以上面的代码为例，<code>gcc --verbose new6.c</code>的关于头文件的搜索路径的输出如下：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;...&quot;</span> search starts here:</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;...&gt;</span> search starts here:</span></span><br><span class="line"> /usr/lib/gcc/x86_64-linux-gnu/<span class="number">11</span>/include</span><br><span class="line"> /usr/local/include</span><br><span class="line"> /usr/include/x86_64-linux-gnu</span><br><span class="line"> /usr/include</span><br><span class="line">End of search <span class="built_in">list</span>.</span><br></pre></td></tr></table></figure>
<p>可以看到，系统头文件搜索路径在 <code>/usr/include</code>
目录下，用户头文件搜索路径在当前目录下。</p>
<h2 id="下划线">下划线</h2>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> FORALL_REGS(_)  _(X) _(Y)</span></span><br></pre></td></tr></table></figure>
<p>上面的宏定义了一个宏 <code>FORALL_REGS</code>，它接受一个参数
<code>_</code>，然后将 <code>X</code> 和 <code>Y</code>
作为参数展开。举个例子：<code>FORALL_REGS(PRINT)</code>, 展开为
<code>PRINT(X) PRINT(Y)</code>。这个其实就是将之前的字母换成了下划线。</p>
<h2 id="运算符">##运算符</h2>
<p>在宏定义中，<code>##</code>
是一个预处理器运算符，称为“标记拼接运算符”。它用于将两个标记（tokens）连接成一个单一的标记。</p>
<p>如：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> DEFIEN(X)       static int X, X##1;</span></span><br></pre></td></tr></table></figure>
<p>当你使用 <code>DEFIEN(foo)</code>
这个宏时，预处理器会将其展开为：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">static</span> <span class="type">int</span> foo, foo1;</span><br></pre></td></tr></table></figure>
<p>这里，<code>X</code> 被替换为 <code>foo</code>，而 <code>X##1</code>
会将 <code>X</code> 和 <code>1</code> 拼接成
<code>foo1</code>。这样可以动态生成变量名，非常有用。</p>
<h1 id="输入输出">输入输出</h1>
<h2 id="打印字符串">打印字符串</h2>
<p>用<code>printf("%s", str)</code>打印字符串时,str是这个字符串首个字符的地址,而不是字符串本身,当打印到结束符<code>\0</code>停止打印。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">char</span> str1[] = <span class="string">&quot;hello world&quot;</span>;</span><br><span class="line">    <span class="type">char</span> str2[] = &#123;<span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;o&#x27;</span>,<span class="string">&#x27;\0&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, <span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;\0&#x27;</span>&#125;;</span><br><span class="line">    <span class="type">char</span> str3[] = &#123;<span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, <span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;\0&#x27;</span>&#125;;</span><br><span class="line">    <span class="type">char</span> str4[] = &#123;<span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, <span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;d&#x27;</span>&#125;;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;str1:   %s\n&quot;</span>, str1);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;str2:   %s\n&quot;</span>, str2);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;str3:   %s\n&quot;</span>, str3);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;str4:   %s\n&quot;</span>, str4);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;&amp;str1[0]: %s\n&quot;</span>, &amp;str1[<span class="number">0</span>]);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;&amp;str2[0]: %s\n&quot;</span>, &amp;str2[<span class="number">0</span>]);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;&amp;str3[0]: %s\n&quot;</span>, &amp;str3[<span class="number">0</span>]);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;&amp;str4[0]: %s\n&quot;</span>, &amp;str4[<span class="number">0</span>]);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;p str1:   %p\n&quot;</span>, str1);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;p str2:   %p\n&quot;</span>, str2);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;p str3:   %p\n&quot;</span>, str3);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;p str4:   %p\n&quot;</span>, str4);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">str1:   hello world</span><br><span class="line">str2:   hello</span><br><span class="line">str3:   helloworld</span><br><span class="line">str4:   helloworldhelloworld</span><br><span class="line">&amp;str1[0]: hello world</span><br><span class="line">&amp;str2[0]: hello</span><br><span class="line">&amp;str3[0]: helloworld</span><br><span class="line">&amp;str4[0]: helloworldhelloworld</span><br><span class="line">p str1:   0x7ffdf492f950</span><br><span class="line">p str2:   0x7ffdf492f95c</span><br><span class="line">p str3:   0x7ffdf492f945</span><br><span class="line">p str4:   0x7ffdf492f93b</span><br></pre></td></tr></table></figure>
<p>像<code>char str1[] = "hello world";</code>这种定义方式，编译器会自动在末尾添加结束符。如果像<code>str4</code>那样定义，结尾没有<code>\0</code>，那么会一直打印直到<code>\0</code>。</p>
<p>我这个虚拟机是小端存储，那么在内存中，这些字符串应该是这样的：</p>
<table>
<thead>
<tr class="header">
<th>address</th>
<th>memory</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>f93b</td>
<td>h</td>
</tr>
<tr class="even">
<td>···</td>
<td></td>
</tr>
<tr class="odd">
<td>f944</td>
<td>d</td>
</tr>
<tr class="even">
<td>f945</td>
<td>h</td>
</tr>
<tr class="odd">
<td>···</td>
<td></td>
</tr>
<tr class="even">
<td>f94f</td>
<td>\0</td>
</tr>
</tbody>
</table>
<p>刚好，<code>str4</code>一直打印，直到遇见<code>str3</code>的<code>\0</code>才停止，所以才是上面的结果。</p>
]]></content>
  </entry>
</search>
