<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>FreeRTOS-1</title>
    <url>/2023/08/27/FreeRTOS-1/</url>
    <content><![CDATA[<p>我是跟着正点原子的教程来学的，感觉讲的可以，因此很多知识点参考了正点原子</p>
<h1 id="为何使用freertos">为何使用FreeRTOS</h1>
<p>裸机在处理任务时，可以采用主函数while循环加中断处理函数的方法，但是这种方法实时性差，代码结构臃肿，并且当某个任务要执行delay()时，CPU只能干等，无法在这段时间去处理其他任务，不能充分利用CPU资源。</p>
<p>FreeRTOS为每个任务划分时间片，优先级。优先级相同的任务在执行时在划分的时间片内执行。在某个任务堵塞(delay)时，该任务进入堵塞状态，CPU可以转去执行其他的任务。</p>
<h1 id="freertos任务调度">FreeRTOS任务调度</h1>
<h2 id="任务调度器">任务调度器</h2>
<p>任务调度器就是通过某种调度算法决定要执行哪一个任务</p>
<h2 id="任务调度方式">任务调度方式</h2>
<ul>
<li>抢占式调度
<ul>
<li>高优先级抢占低优先级</li>
<li><strong>被抢占的任务会进入就绪态</strong></li>
<li>高优先级不结束，低优先级不能执行</li>
</ul></li>
<li>时间片调度
<ul>
<li><strong>优先级相同</strong>的任务会划分时间片，</li>
<li><strong>每个任务执行一个时间片</strong>，执行完后不断流转</li>
<li><strong>时间片的大小取决于系统滴答定时器</strong></li>
<li>倘若某个任务有一楼最好原因只执行0.2个时间片，那么在后面的运行中这个缺少的时间片并不会补回来，还是按照一个时间片执行</li>
</ul></li>
<li>协程式调度</li>
</ul>
<h2 id="任务状态">任务状态</h2>
<ul>
<li>运行态：某个任务正在被执行，那么该任务就处于运行态。在STM32中，只有一个任务处于运行态</li>
<li>就绪态：某个任务可以被执行，但是还未被执行，那么就处于就绪态</li>
<li>阻塞态：某任务在等待延时或者外部事件，则处于阻塞态</li>
<li>挂起态：类似于暂停，使用函数vTaskSuspend()挂起，使用vTaskResume()解挂，进入就绪态</li>
</ul>
<div data-align="center">
<p><img src="/2023/08/27/FreeRTOS-1/FreeRTOS-1-1.png" height="360"></p>
</div>
<p>由上图可知，<strong>仅有就绪态可以转变成运行态；任何任务想要转变为运行态，都要先转变成就绪态</strong></p>
<h2 id="任务列表">任务列表</h2>
<p>除运行态外，其他三种状态都有任务列表</p>
<ul>
<li>就绪列表</li>
</ul>
<table>
<thead>
<tr class="header">
<th>标志位</th>
<th>任务列表</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>bit31</td>
<td>pxReadyTasksLists[31]</td>
</tr>
<tr class="even">
<td>bit30</td>
<td>pxReadyTasksLists[30]</td>
</tr>
<tr class="odd">
<td>·····</td>
<td>·····················</td>
</tr>
<tr class="even">
<td>bit1</td>
<td>pxReadyTasksLists[1]</td>
</tr>
<tr class="odd">
<td>bit0</td>
<td>pxReadyTasksLists[0]</td>
</tr>
</tbody>
</table>
<p>标志位置一表示该就绪态优先级列表有任务，反之无任务。</p>
<p>数字越大，优先级等级越高。这一点与STM32中断优先级的高低不一样。</p>
<p>任务调度器就是在就绪列表中选择优先级最高的任务来执行，即进入运行态。</p>
<ul>
<li>阻塞列表：pxDelayedTaskList</li>
<li>挂起列表：xSuspendedTaskList</li>
</ul>
]]></content>
      <categories>
        <category>STM32</category>
      </categories>
      <tags>
        <tag>FreeRTOS</tag>
        <tag>STM32</tag>
        <tag>C语言</tag>
      </tags>
  </entry>
  <entry>
    <title>FreeRTOS-2</title>
    <url>/2023/09/02/FreeRTOS-2/</url>
    <content><![CDATA[<h1 id="任务操作相关的api函数">任务操作相关的API函数</h1>
<table>
<thead>
<tr class="header">
<th>API函数</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>xTaskCreate()</td>
<td>动态方式创建任务</td>
</tr>
<tr class="even">
<td>xTaskCreateStatic()</td>
<td>静态方式创建任务</td>
</tr>
<tr class="odd">
<td>xTaskDelete()</td>
<td>删除任务</td>
</tr>
</tbody>
</table>
<p>由xTaskCreate()创建的任务，在创建成功后立马进入就绪态。被删除的任务将从所有的列表删除。动态创建方式是由FreeRTOS进行内存管理，静态则是由人工进行管理。静态创建比较麻烦，因此大多使用动态创建。</p>
<p>若使用静态创建，需要将宏<code>configSUPPORT_STATIC_ALLOCATION</code>置为1。同样若为动态创建，则需将宏<code>configSUPPORT_DYNAMIC_ALLOCATION</code>置为1。</p>
<h2 id="xtaskcreate函数">xTaskCreate()函数</h2>
<p>函数原型</p>
<div data-align="center">
<p><img src="/2023/09/02/FreeRTOS-2/FreeRTOS-2-1.png" height="360"></p>
</div>
<p>形参描述</p>
<table>
<colgroup>
<col style="width: 15%">
<col style="width: 84%">
</colgroup>
<thead>
<tr class="header">
<th>形参</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>pxTaskCode</td>
<td>指向任务函数的指针，即任务函数的名字</td>
</tr>
<tr class="even">
<td>pcName</td>
<td>任务函数的名字</td>
</tr>
<tr class="odd">
<td>usStackDepth</td>
<td>任务堆栈大小，单位：字</td>
</tr>
<tr class="even">
<td>pvParameters</td>
<td>传递给任务函数的参数</td>
</tr>
<tr class="odd">
<td>uxPriority</td>
<td>任务函数优先级</td>
</tr>
<tr class="even">
<td>pxCreatedTask</td>
<td>任务句柄，任务成功创建后，会返回任务句柄。任务句柄就是任务的任务控制块</td>
</tr>
</tbody>
</table>
<p>返回值</p>
<table>
<thead>
<tr class="header">
<th>返回值</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>pdPASS</td>
<td>任务创建成功</td>
</tr>
<tr class="even">
<td>errCOULD_NOT_ALLOCATE_REQUIRED_MEMORY</td>
<td>内存不足，任务创建失败</td>
</tr>
</tbody>
</table>
<h2 id="xtaskcreatestatic函数">xTaskCreateStatic()函数</h2>
<p>函数原型</p>
<div data-align="center">
<p><img src="/2023/09/02/FreeRTOS-2/FreeRTOS-2-2.png" height="360"></p>
</div>
<p>形参描述</p>
<table>
<colgroup>
<col style="width: 12%">
<col style="width: 87%">
</colgroup>
<thead>
<tr class="header">
<th>形参</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>pxTaskCode</td>
<td>指向任务函数的指针，即任务函数的名字</td>
</tr>
<tr class="even">
<td>pcName</td>
<td>任务函数的名字</td>
</tr>
<tr class="odd">
<td>ulStackDepth</td>
<td>任务堆栈大小，单位：字</td>
</tr>
<tr class="even">
<td>pvParameters</td>
<td>传递给任务函数的参数</td>
</tr>
<tr class="odd">
<td>uxPriority</td>
<td>任务函数优先级</td>
</tr>
<tr class="even">
<td>puxStackBuffer</td>
<td>任务栈指针，内存由用户分配提供,就是定义一个数组，数组的名字就是这个指针，数组的大小就是任务堆栈大小</td>
</tr>
<tr class="odd">
<td>pxTaskBuffer</td>
<td>任务控制块指针，内存由用户分配提供</td>
</tr>
<tr class="even">
<td>pxCreatedTask</td>
<td>任务句柄，任务成功创建后，会返回任务句柄。任务句柄就是任务的任务控制块</td>
</tr>
</tbody>
</table>
<p>返回值</p>
<table>
<thead>
<tr class="header">
<th>返回值</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>NULL</td>
<td>用户没有提供相应的内存，任务创建失败</td>
</tr>
<tr class="even">
<td>其他值</td>
<td>任务句柄，创建成功</td>
</tr>
</tbody>
</table>
<h2 id="vtaskdelete函数">vTaskDelete()函数</h2>
<table>
<thead>
<tr class="header">
<th>形参</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>vTaskToDelete</td>
<td>待删除的任务的句柄</td>
</tr>
</tbody>
</table>
<p>当传入的实参为<code>NULL</code>时，代表删除任务自身</p>
<p>该函数无返回值</p>
<h1 id="命名规范">命名规范</h1>
<ul>
<li>u: unsigned</li>
<li>s: short</li>
<li>l: long</li>
<li>c: char</li>
<li>x:
用户自定义的数据类型，如结构体，队列等。表示的类型为BaseType_t。如函数<code>xTaskCreate()</code>，其返回值就是BaseType_t类型</li>
<li>e: 枚举</li>
<li>p: 指针</li>
<li>prv: static函数</li>
<li>v: void函数，无返回值，如<code>vTaskDelete()</code></li>
</ul>
<p>函数名包含了该函数的返回值、函数所在的文件、函数功能。</p>
<p>如<code>xTaskCreateStatic()</code>，其返回值类型为BaseType_t，在task.c文件中，功能是静态创建任务。</p>
<p>参考：<code>https://blog.csdn.net/freestep96/article/details/126692753</code></p>
]]></content>
      <categories>
        <category>STM32</category>
      </categories>
      <tags>
        <tag>FreeRTOS</tag>
        <tag>STM32</tag>
        <tag>C语言</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习1</title>
    <url>/2023/08/12/MachineLearning-1/</url>
    <content><![CDATA[<h1 id="线性模型">线性模型</h1>
<hr>
<h2 id="机器学习machine-learning几个步骤">机器学习(machine
learning)几个步骤</h2>
<ol type="1">
<li>准备数据集(dataset)</li>
<li>选择模型(model)</li>
<li>训练(training)</li>
<li>应用(inferring)</li>
</ol>
<p>数据集分为两部分：训练集、测试集。</p>
<p>训练集又可以细分为训练集、开发集</p>
<p>损失函数是针对一个样本的,平均平方误差(Mean Square
Error,mse)是针对于整个训练集。</p>
<p>训练神经网络本质上就是使用几个或者一些参数将一个模型变换为更加复杂的模型</p>
<p>损失函数的选择很重要，因为它是一种对训练样本中要修正的错误进行优先处理的方法，可以强调或者忽略某些误差</p>
<h2 id="过拟合">过拟合</h2>
<p>用训练集去训练模型，在尽可能的使损失最小后，将模型在测试集验证时发现，模型产生的损失比预期的要高得多，即过拟合</p>
<div data-align="center">
<p><img src="/2023/08/12/MachineLearning-1/MachineLearning-1-3.jpg" height="360"> 引自《Deep
Learning with PyTorch》</p>
</div>
<p>解决过拟合的方法 1.
在损失函数中添加惩罚项，以降低模型的成本，使其表现得更加平稳、变换更缓慢
2.
在输入样本中添加噪声，人为地在训练数据样本之间创建新的数据点，并使模型也拟合这些点
3. ···</p>
<p>那么现在可以将训练神经网络（选择合适参数）的过程分为两步：增大参数直到拟合；缩小参数以避免出现过拟合</p>
<h2 id="练习">练习</h2>
<h3 id="question">Question</h3>
<p>Suppose that students would get y points in final exam, if they spent
x hours in study</p>
<table>
<thead>
<tr class="header">
<th>x</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>2</td>
</tr>
<tr class="even">
<td>2</td>
<td>4</td>
</tr>
<tr class="odd">
<td>3</td>
<td>6</td>
</tr>
<tr class="even">
<td>4</td>
<td>?</td>
</tr>
</tbody>
</table>
<h3 id="answer">Answer</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">	</span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">x_data = &#123;<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>&#125;</span><br><span class="line">y_data = &#123;<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x, y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) * (y_pred - y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#权重及其对应损失值</span></span><br><span class="line">w_list = []</span><br><span class="line">mse_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> np.arange(<span class="number">0.0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;w=&#x27;</span>,w)</span><br><span class="line">    l_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        y_pred_val = forward(x_val)</span><br><span class="line">        loss_val = loss(x_val, y_val)</span><br><span class="line">        l_sum += loss_val</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\t&#x27;</span>, x_val, y_val, y_pred_val, loss_val)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;MSE=&#x27;</span>, l_sum/<span class="number">3</span>)</span><br><span class="line">    w_list.append(w)</span><br><span class="line">    mse_list.append(l_sum/<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(w_list, mse_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div data-align="center">
<p><img src="/2023/08/12/MachineLearning-1/MachineLearning-1-1.png" height="360" title="y=x*w" alt="y=x*w">
y=x*w</p>
</div>
<hr>
<h3 id="question-1">Question</h3>
<p>Suppose that students would get y points in final exam, if they spent
x hours in study.Try to use the model y=x*w+b, and draw the cost
graph.</p>
<table>
<thead>
<tr class="header">
<th>x</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>2</td>
</tr>
<tr class="even">
<td>2</td>
<td>4</td>
</tr>
<tr class="odd">
<td>3</td>
<td>6</td>
</tr>
<tr class="even">
<td>4</td>
<td>?</td>
</tr>
</tbody>
</table>
<h3 id="answer-1">Answer</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">x_data = &#123;<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>&#125;</span><br><span class="line">y_data = &#123;<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * w + b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x, y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) * (y_pred - y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 权重及其对应损失值</span></span><br><span class="line">w_list = []</span><br><span class="line">b_list = []</span><br><span class="line">mse_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> np.arange(<span class="number">0.0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>):</span><br><span class="line">    <span class="keyword">for</span> b <span class="keyword">in</span> np.arange(-<span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">0.1</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;w=&#x27;</span>,  w, <span class="string">&#x27;b=&#x27;</span>, b)</span><br><span class="line">        l_sum = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">            y_pred_val = forward(x_val)</span><br><span class="line">            loss_val = loss(x_val, y_val)</span><br><span class="line">            l_sum += loss_val</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;\t&#x27;</span>, x_val, y_val, y_pred_val, loss_val)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;MSE=&#x27;</span>, l_sum/<span class="number">3</span>)</span><br><span class="line">        w_list.append(w)</span><br><span class="line">        b_list.append(b)</span><br><span class="line">        mse_list.append(l_sum/<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mse_list)</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax3d = fig.add_subplot(projection=<span class="string">&#x27;3d&#x27;</span>)  <span class="comment"># 创建三维坐标系</span></span><br><span class="line">ax3d.plot_trisurf(w_list, b_list, mse_list)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div data-align="center">
<p><img src="/2023/08/12/MachineLearning-1/MachineLearning-1-2.png" height="360" title="y=x*w+b" alt="y=x*w+b">
y=x*w+b</p>
</div>
<hr>
<p>matplotlib中的函数还不怎么会用，后面抽个时间看一看</p>
<blockquote>
<p>https://blog.csdn.net/hustlei/article/details/122408179</p>
</blockquote>
<p>这个博客里面思维导图可以看一看捏</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习4</title>
    <url>/2023/08/20/MachineLearning-4/</url>
    <content><![CDATA[<h1 id="用pytorch实现线性回归linear-regression">用pytorch实现线性回归(linear
regression)</h1>
<p>使用梯度下降关键是求出损失函数对于权重的导数</p>
<h2 id="步骤">步骤</h2>
<ol type="1">
<li>准备数据集</li>
<li>设计模型</li>
<li>构建损失函数、优化器</li>
<li>训练周期</li>
</ol>
<h2 id="训练的步骤">训练的步骤</h2>
<ol type="1">
<li>求y^（预测结果）</li>
<li>求损失函数</li>
<li>backward</li>
<li>更新</li>
</ol>
<p>问题同前几篇blog</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">2.0</span>], [<span class="number">4.0</span>], [<span class="number">6.0</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># 构造对象，包含权重与偏置</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y_pred = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = LinearModel()<span class="comment">#可调用</span></span><br><span class="line"><span class="comment">#损失值应该是标量</span></span><br><span class="line">criterion = torch.nn.MSELoss(size_average=<span class="literal">False</span>)  <span class="comment"># false为不求平均值，即不除以N</span></span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)  <span class="comment"># lr,学习率</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    <span class="built_in">print</span>(epoch, loss.item())</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()  <span class="comment"># 进行更新</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;w = &quot;</span>, model.linear.weight.item())  <span class="comment"># weight是矩阵</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b = &quot;</span>, model.linear.bias.item())</span><br><span class="line"></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_pred = &quot;</span>, y_test.data)</span><br></pre></td></tr></table></figure>
<p>类名必须用大驼峰命名</p>
<p>要把模型定义成一个类，模型类都要继承于torch.nn.Model</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习2</title>
    <url>/2023/08/13/MachineLearning-2/</url>
    <content><![CDATA[<h1 id="梯度下降算法">梯度下降算法</h1>
<hr>
<p>采用分治的思路，不断划分区块，进行搜索，但是会有可能陷入局部最优解</p>
<p>梯度：目标函数对权重求导。导数为负的方向就是最小指的方向</p>
<p>在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率
在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向</p>
<p>即：<code>ω = ω + α*θ</code>。θ为梯度，α为学习率或步长</p>
<p>类似于贪心算法，只看眼前最好的选择，不一定能得到最优结果，但能得到局部最优结果</p>
<p>鞍点：导数为零</p>
<hr>
<h2 id="随机梯度下降算法">随机梯度下降算法</h2>
<p>梯度下降衍生版本：随机梯度下降（stochastic gradient
descent）。随机选择单个样本的损失函数求导。可以避免陷入鞍点。SGD算法是从样本中随机抽出一组，训练后按梯度更新一次，然后再抽取一组，再更新一次，在样本量及其大的情况下，可能不用训练完所有的样本就可以获得一个损失值在可接受范围之内的模型了。</p>
<hr>
<p>针对于MachineLearning-1的问题，梯度下降算法代码如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">x_data = &#123;<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>&#125;</span><br><span class="line">y_data = &#123;<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始权重猜测</span></span><br><span class="line">w = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cost</span>(<span class="params">xs, ys</span>):</span><br><span class="line">    cost = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(xs, ys):</span><br><span class="line">        y_pred = forward(x)</span><br><span class="line">        cost += (y_pred - y) ** <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> cost / <span class="built_in">len</span>(xs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">xs, ys</span>):</span><br><span class="line">    grad = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(xs, ys):</span><br><span class="line">        grad += <span class="number">2</span> * x * (x * w - y)</span><br><span class="line">    <span class="keyword">return</span> grad / <span class="built_in">len</span>(xs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict (before training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    cost_val = cost(x_data, y_data)</span><br><span class="line">    grad_val = gradient(x_data, y_data)</span><br><span class="line">    w -= <span class="number">0.01</span> * grad_val</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch:&#x27;</span>, epoch, <span class="string">&#x27;w=&#x27;</span>, w, <span class="string">&#x27;loss=&#x27;</span>, cost_val)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict (after training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<p>随机梯度下降代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">x_data = &#123;<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>&#125;</span><br><span class="line">y_data = &#123;<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始权重猜测</span></span><br><span class="line">w = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x, y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">xs, ys</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * x * (x * w - y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict (before training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        grad = gradient(x_data, y_data)</span><br><span class="line">        w -= <span class="number">0.01</span> * grad</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\tgrad:&#x27;</span>, x, y, grad)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;progress:&#x27;</span>, epoch, <span class="string">&#x27;w=&#x27;</span>, w, <span class="string">&#x27;loss=&#x27;</span>, loss(x, y))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict (after training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<p>梯度下降算法可以并行化，随机梯度下降不行</p>
<p>mini-batch：小批量随机梯度下降。如果数据样本的数量很大，那么一轮的迭代会很耗时间，所以可以把这些数据样本划分为若干份，这些子集就叫做mini—batch</p>
<p>有关随机梯度下降文章 &gt;
https://blog.csdn.net/qq_58146842/article/details/121280968?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86&amp;utm_medium=distribute.pc_search_result.none-task-blog-2<sub>all</sub>sobaiduweb~default-1-121280968.nonecase&amp;spm=1018.2226.3001.4187</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习3</title>
    <url>/2023/08/17/MachineLearning-3/</url>
    <content><![CDATA[<h1 id="反向传播back-propagation">反向传播（Back Propagation）</h1>
<p>权重太多，求解析式十分复杂，因此在面对复杂网络时，尝试把网络看作是一个图，根据链式法则(chain
rule)，求其解析式，即反向传播算法。</p>
<p>Back Propagation链式求导过程 1. Create Computational Graph 2. Local
Gradient 3. Given gradient from successive node 4. Use chain rule to
compute the gradient(Backward)</p>
<p>Tensor
张量，pytorch里的重要组成，可以是标量，也可以是向量，矩阵。它包含数据（data），导数（grad），即权重与损失函数对权重的导数。</p>
<p>正向的目标是求出本次的损失，反向的目标则是求出导数。蓝色为正向，红色为反向</p>
<div data-align="center">
<p><img src="/2023/08/17/MachineLearning-3/MachineLearning-3-1.png" height="360"></p>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个Tensor变量，权重</span></span><br><span class="line">w = torch.Tensor([<span class="number">1.0</span>])</span><br><span class="line"><span class="comment"># 需要计算梯度</span></span><br><span class="line">w.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型，此时w是Tensor，*要计算Tensor与Tensor之间的乘法，所以有一个对x的自动类型强转</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数，构建计算图</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x, y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;predict (before training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br><span class="line"><span class="comment"># w.grad也是个梯度，w.grad.item才是标量</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        l = loss(x, y)  <span class="comment"># l是个张量，前馈过程，计算loss</span></span><br><span class="line">        l.backward()<span class="comment">#计算梯度</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\tgrad:&quot;</span>, x, y, w.grad.item())</span><br><span class="line">        w.data = w.data - <span class="number">0.01</span> * w.grad.data<span class="comment">#梯度用于更新权重</span></span><br><span class="line">        w.grad.data.zero_()  <span class="comment"># 梯度清零</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;process:&quot;</span>, epoch, l.item())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;predict(after training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br></pre></td></tr></table></figure>
<p>需要注意的是每一次通过.backward()计算的梯度会累积，因此在更新后，需要通过.grad.data.zero()将梯度置零</p>
<p>权重是指w，梯度是指loss对w的导数。要先通过前向过程求出loss，求出loss后通过反向传播求出loss对w的导数,再根据随机梯度下降算法或者其他算法对权重w进行更新。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习5</title>
    <url>/2023/08/21/MachineLearning-5/</url>
    <content><![CDATA[<h1 id="逻辑斯蒂回归logistic-regression">逻辑斯蒂回归（logistic
regression)</h1>
<div data-align="center">
<p><img src="/2023/08/21/MachineLearning-5/MachineLearning-5-2.png" height="360"></p>
</div>
<p>名字虽然是回归，但实际上是分类</p>
<p>回归问题的预测结果范围是实数，而分类问题的预测结果为0~1，因此需要映射</p>
<p>可以使用Sigmoid函数(s型函数)，将线性模型的结果压缩，比如：</p>
<p><span class="math display">\[
f(x)=\frac{1}{1+{\rm e}^{-x}}
\]</span></p>
<p>那么预测结果(0~1)就为</p>
<p><span class="math display">\[
\hat{y} = f(w·x+b)=\frac{1}{1+{\rm e}^{-(w·x+b)}}
\]</span></p>
<div data-align="center">
<p><img src="/2023/08/21/MachineLearning-5/MachineLearning-5-1.png" height="360"></p>
</div>
<!--
## 逻辑斯蒂回归模型

<div align=center>
<img src="MachineLearning-5-3.png" height = '360'>
</div>

最后的判别结果是通过比较P(Y=1|x)和P(Y=0|x)的大小来确定的，若Y=1大，那么Y=1，反之Y=0

<p /hidden>
## 损失函数
<div align=center>
<img src="MachineLearning-5-5.png" height = '360'>
</div>

<div align=center>
<img src="MachineLearning-5-4.png" height = '360'>
</div>

### 对数似然函数

似然函数可以理解成通过已知的结果去倒推出最大概率得到该结果的参数，即“模型已定，参数未知”。就比如已知一些数据，我们对这个模型使用正态分布，那么我们就可以求出与已知的数据对应最好的那条拟合曲线，求出均值μ，标准差σ。

对于逻辑斯蒂回归,近似看成二项分布

$$
p(Y=Y_i|x_i,w) = 
\begin{cases}
[π(x_i)]^{Y_i}      & Y_i=1 \\
[1-π(x_i)]^{1-Y_i}] & Y_i=0 \\
\end{cases}
$$

则有
$$
p(Y=Y_i|x_i,w) = π(x_i)^{Y_i}·[1-π(x_i)]^{1-Y_i}]
$$

当有m个样本时
$$
P = \prod_{i=1}^m [π(x_i)^{Y_i}·[1-π(x_i)]^{1-Y_i}]
$$

对其取对数

$$
\begin{aligned}
L(w) &= \log_{e}{\prod_{i=1}^m [π(x_i)^{Y_i}·[1-π(x_i)]^{1-Y_i}]} \\
&= \sum_{i=1}^m [ {Y_i} log_{e}π(x_i) + (1-Y_i)log_{e}[1-π(x_i)] ] 
\end{aligned}
$$
-->
<h2 id="损失函数">损失函数</h2>
<div data-align="center">
<p><img src="/2023/08/21/MachineLearning-5/MachineLearning-5-6.png" height="360"></p>
</div>
<p>损失函数不再使用MSE，采用下面的式子</p>
<ul>
<li>当预测结果 <span class="math inline">\(\hat{y}=1\)</span>，实际结果为<span class="math inline">\(1\)</span>时，<span class="math inline">\(loss=0\)</span></li>
<li>当预测结果 <span class="math inline">\(\hat{y}=0\)</span>，实际结果为<span class="math inline">\(1\)</span>时，<span class="math inline">\(loss=+\infty\)</span></li>
<li>当预测结果 <span class="math inline">\(\hat{y}=1\)</span>，实际结果为<span class="math inline">\(0\)</span>时，<span class="math inline">\(loss=-\infty\)</span></li>
<li>当预测结果 <span class="math inline">\(\hat{y}=0\)</span>，实际结果为<span class="math inline">\(0\)</span>时，<span class="math inline">\(loss=0\)</span></li>
</ul>
<h2 id="代码">代码</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">0</span>], [<span class="number">0</span>], [<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LogisticRegreesionModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LogisticRegreesionModel, self).__init__()</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># 构造对象，包含权重与偏置</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y_pred = F.sigmoid(self.linear(x))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = LogisticRegreesionModel()</span><br><span class="line"></span><br><span class="line">criterion = torch.nn.BCELoss(size_average=<span class="literal">False</span>)  <span class="comment"># false为不求平均值，即不除以N</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)  <span class="comment"># 优化器 lr,学习率</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    <span class="built_in">print</span>(epoch, loss.item())</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()  <span class="comment"># 进行更新</span></span><br><span class="line"></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_pred = &quot;</span>, y_test.data)</span><br></pre></td></tr></table></figure>
<p>如果用matplotlib绘图，也可以看见输出结果<span class="math inline">\(\hat{y}\)</span>为S型曲线</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习6</title>
    <url>/2023/08/29/MachineLearning-6/</url>
    <content><![CDATA[<h1 id="多维输入">多维输入</h1>
<p>对于标量或者一维的回归任务，其回归模型可以表示为<span class="math inline">\(\hat{y}=\sigma(x^{(i)}·w+b)\)</span>。其中<span class="math inline">\(\sigma\)</span>是激活函数，类似于sigmoid函数，i为第i个样本</p>
<p>但对于多维的数据，例如有这么一个样本</p>
<table>
<thead>
<tr class="header">
<th>x1</th>
<th>x2</th>
<th>x3</th>
<th>x4</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>10</td>
<td>11</td>
<td>21</td>
<td>31</td>
<td>234</td>
</tr>
<tr class="even">
<td>10</td>
<td>11</td>
<td>21</td>
<td>31</td>
<td>234</td>
</tr>
<tr class="odd">
<td>10</td>
<td>11</td>
<td>21</td>
<td>31</td>
<td>234</td>
</tr>
</tbody>
</table>
<p>对于这么一组数据来说，x1、x2、x3、x4被称为<strong>特征(feature)</strong>。那么回归模型的输入就是一组向量:</p>
<p><span class="math display">\[
\left[
\begin{matrix}
    x_1 &amp; x_2 &amp;x_3 &amp; x_4
\end{matrix}
\right]
\]</span></p>
<p>回归模型变为<span class="math inline">\(\hat{y}=\sigma(\sum_{n=1}^{4}
x{_n^{(i)}}·w_n+b)=\sigma(z^{(i)})\)</span>，<span class="math inline">\(x{_n^{(i)}}\)</span>表示第i个样本的<span class="math inline">\(x_n\)</span>。其中的w·x虽然表示的是标量相乘，但是实际的含义是矩阵相乘，即</p>
<p><span class="math display">\[
\left[
\begin{matrix}
    x_1 &amp; x_2 &amp; x_3 &amp; x_4
\end{matrix}
\right]
\left[
\begin{matrix}
    w_1\\
    w_2\\
    w_3\\
    w_4\\
\end{matrix}
\right]
\]</span></p>
<p>计算结果依旧是标量</p>
<h1 id="对于mini-batchn-samples">对于mini-batch(N samples)</h1>
<p>回归模型依旧是<span class="math inline">\(\hat{y}=\sigma(\sum_{n=1}^{N}
x{_n^{(i)}}·w_n+b)=\sigma(z^{(i)})\)</span></p>
<p>现在有N个样本</p>
<p><span class="math display">\[
\left[
\begin{matrix}
    {\hat{y}}^{(1)} \\
    \vdots \\
    {\hat{y}}^{(N)} \\
\end{matrix}
\right] =
\left[
\begin{matrix}
    \sigma(z^{(1)}) \\
    \vdots \\
    \sigma(z^{(N)}) \\
\end{matrix}
\right] =
\sigma
\left[
\begin{matrix}
    z^{(1)} \\
    \vdots \\
    z^{(N)} \\
\end{matrix}
\right]
\]</span></p>
<p><span class="math display">\[
z^{(N)}=\left[
\begin{matrix}
    x^{(N)}_1 &amp; \cdots &amp; \cdots &amp; x^{(N)}_4
\end{matrix}
\right]
\left[
\begin{matrix}
    w_1 \\
    \vdots \\
    \vdots \\
    w_4
\end{matrix}
\right]+b
\]</span></p>
<p>计算结果为标量，转置不影响,其中4是feature数量</p>
<p>那么我们就可以得到</p>
<p><span class="math display">\[
\left[
\begin{matrix}
    z^{(1)} \\
    \vdots \\
    z^{(N)} \\
\end{matrix}
\right]=
\left[
\begin{matrix}
    x^{(1)}_1 &amp; \cdots &amp; x^{(1)}_4\\
    \vdots    &amp; \ddots &amp; \vdots   \\
    x^{(N)}_1 &amp; \cdots &amp; x^{(N)}_4
\end{matrix}
\right]
\left[
\begin{matrix}
    w_1 \\
    \vdots \\
    \vdots \\
    w_4
\end{matrix}
\right]+
\left[
\begin{matrix}
    b \\
    \vdots \\
    b\\
\end{matrix}
\right]
\]</span></p>
<p>设样本数量为N，feature的数量为n，则</p>
<table>
<thead>
<tr class="header">
<th>x</th>
<th>w</th>
<th>b</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>N×n</td>
<td>n×1</td>
<td>N×1</td>
</tr>
</tbody>
</table>
<p>转成向量化的计算后，可利用并行化计算，提高计算速率。</p>
<p>在改代码时，只需将<code>self.linear = torch.nn.Linear(1, 1)</code>改成<code>self.linear = torch.nn.Linear(n, 1)</code></p>
<p>如果是多层网络，那么只需将上一层的输出，作为下一层的输入。</p>
<div data-align="center">
<p><img src="/2023/08/29/MachineLearning-6/MachineLearning-6-1.png" height="360"></p>
</div>
<h1 id="实际问题">实际问题</h1>
<div data-align="center">
<p><img src="/2023/08/29/MachineLearning-6/MachineLearning-6-2.png" height="360"></p>
</div>
<p>X1~X8为糖尿病病人的一系列指标，Y表示一年后病情是否加重。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">xy = np.loadtxt(</span><br><span class="line">    <span class="string">&quot;D:\BaiduNetdiskDownload\PyTorch深度学习实践\diabetes.csv.gz&quot;</span>,</span><br><span class="line">    delimiter=<span class="string">&quot;,&quot;</span>,</span><br><span class="line">    dtype=np.float32,</span><br><span class="line">)</span><br><span class="line">x_data = torch.from_numpy(xy[:, :-<span class="number">1</span>])</span><br><span class="line">y_data = torch.from_numpy(xy[:, [-<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(<span class="number">8</span>, <span class="number">6</span>)  <span class="comment"># 8维到6维</span></span><br><span class="line">        self.linear2 = torch.nn.Linear(<span class="number">6</span>, <span class="number">4</span>)  <span class="comment"># 6维到4维</span></span><br><span class="line">        self.linear3 = torch.nn.Linear(<span class="number">4</span>, <span class="number">1</span>)  <span class="comment"># 4维到1维</span></span><br><span class="line">        self.activate = torch.nn.Sigmoid()</span><br><span class="line">        <span class="comment"># self.activate=torch.nn.ReLU()#采用不同的激活函数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.activate(self.linear1(x))</span><br><span class="line">        x = self.activate(self.linear2(x))</span><br><span class="line">        x = self.activate(self.linear3(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Model()</span><br><span class="line"></span><br><span class="line">criterion = torch.nn.BCELoss(size_average=<span class="literal">True</span>)  <span class="comment"># false为不求平均值，即不除以N</span></span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)  <span class="comment"># lr,学习率</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># 前馈</span></span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    <span class="built_in">print</span>(epoch, loss.item())</span><br><span class="line">    <span class="comment"># 反馈</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 更新</span></span><br><span class="line">    optimizer.step()  <span class="comment"># 进行更新</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x_test = torch.Tensor([[-<span class="number">0.29</span>, <span class="number">0.49</span>, <span class="number">0.18</span>, -<span class="number">0.29</span>, <span class="number">0.00</span>, <span class="number">0.00</span>, -<span class="number">0.53</span>, -<span class="number">0.03</span>]])</span><br><span class="line"></span><br><span class="line">y_test = model(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_pred = &quot;</span>, y_test.data)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>OV2640</title>
    <url>/2023/10/08/OV2640/</url>
    <content><![CDATA[<h1 id="dcmi">DCMI</h1>
<h1 id="sccbserial-camera-control-bus协议">SCCB(Serial Camera Control
Bus)协议</h1>
<p>SCCB与I2C协议类似，由两根线组成：SCL(SIO_C)(时钟线)、SDA(SIO_D)(数据线)。值得注意的是，在SCCB的手册(我看的是《OmniVision
Technologies Seril Camera Control Bus(SCCB)
Specification》)上，有说SCCB是有三根线的，另外一根SCCB_E<del>应该是作为片选信号</del>，用来表明信号的开始传输与结束传输，这种方式支持一个主机与多个从机连接。如果只用两根线的话，那就只能支持一主机一从机。</p>
<p>相同点： - 起始信号、停止信号 -
信号有效性(SDA只能在SCL为低电平时变化)</p>
<p>不同点： -
在第九个周期，I2C的SDA内容为应答信号；SCCB的SDA在写周期为Don't-Care位，在读周期为NA位
- SCCB传输单位：相</p>
<p>在SCCB中，数据传输的最基本单位叫做<code>相(phase)</code>。一个相包含9个位，前八个位是八位连续数据，第九个位是<code>Don't Care</code>或者<code>NA</code>，这个到底是哪一个取决于是读还是写。</p>
<div data-align="center">
<p><img src="/2023/10/08/OV2640/OV2640-1.png" height="360"></p>
</div>
<p>phase-1 ID
Address是指从机的地址，bit7-bit1是从机的地址，bit0是读写位(0-write
1-read)。Sub-address 是内存地址。</p>
<div data-align="center">
<p><img src="/2023/10/08/OV2640/OV2640-2.png" height="360"></p>
</div>
<p>写操作：三相写传输周期</p>
<ul>
<li>设备写通信地址，内存地址，所写数据</li>
</ul>
<p>读操作</p>
<ul>
<li>两相写传输周期:设备写通信地址，内存地址</li>
</ul>
<div data-align="center">
<p><img src="/2023/10/08/OV2640/OV2640-3.png" height="360"></p>
</div>
<ul>
<li>两相读传输周期：设备读通信地址，所读数据</li>
</ul>
<div data-align="center">
<p><img src="/2023/10/08/OV2640/OV2640-4.png" height="360"></p>
</div>
<p>需要注意的是SCCB不支持连续读写，只能写一个数据读一个数据</p>
<p>VSYNC:帧信号，摄像头输出，用于标记一帧数据的开始与结束</p>
<p>HREF:行中断信号，摄像头输出，用于标记一行数据的开始与结束</p>
]]></content>
  </entry>
</search>
